{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from __future__ import print_function\n",
    "import pickle\n",
    "import os\n",
    "import IPython\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn.mixture\n",
    "\n",
    "# Path for all pre-computed chroma\n",
    "DATA_DIR = '/home/py/projects/dataset/beatles/beatchromlabs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the list of training file IDs.\n",
    "def read_file_list(filename):\n",
    "    \"\"\"Read a text file with one item per line.\"\"\"\n",
    "    items = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            items.append(line.strip())\n",
    "    return items\n",
    "\n",
    "def read_beat_chroma_labels(file_id):\n",
    "    \"\"\"Read back a precomputed beat-synchronous chroma record.\"\"\"\n",
    "    filename = os.path.join(os.path.join(DATA_DIR, 'beatchromlabs', file_id + '.pkl'))\n",
    "    with open(filename, \"rb\") as f:\n",
    "        u = pickle._Unpickler(f)\n",
    "        u.encoding = 'latin1'\n",
    "        beat_times, chroma_features, label_indices = u.load()\n",
    "        # beat_times, chroma_features, label_indices = pickle.load(f)\n",
    "    #chroma_features = chroma_features**0.25\n",
    "    chroma_features /= np.maximum(0.01, np.max(chroma_features, axis=1))[:, np.newaxis]\n",
    "    return beat_times, chroma_features, label_indices\n",
    "\n",
    "def load_all_features_labels(train_ids):\n",
    "    \"\"\"Load all the features and labels from a lit into big arrays.\"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    for train_id in train_ids:\n",
    "        beat_times, chroma, label = read_beat_chroma_labels(train_id)\n",
    "        assert not np.any(np.isnan(chroma))\n",
    "        features.append(chroma)\n",
    "        labels.append(label)\n",
    "    features = np.concatenate(features)\n",
    "    labels = np.concatenate(labels)\n",
    "    print('Training features shape:', features.shape)\n",
    "    return features, labels\n",
    "\n",
    "def estimate_transitions(labels, num_models):\n",
    "    # Count the number of transitions in the label set.\n",
    "    # Each element of gtt is a 4 digit number indicating one transition \n",
    "    # e.g. 2400 for 24 -> 0.\n",
    "    hashed_transitions = 100*labels[:-1] + labels[1:]\n",
    "    # Arrange these into the transition matrix by counting each type.\n",
    "    transitions = np.zeros((num_models, num_models))\n",
    "    for i in range(num_models):\n",
    "        for j in range(num_models):\n",
    "            transition_hash = 100 * i + j \n",
    "            # Add one to all counts, so no transitions have zero \n",
    "            # probability.\n",
    "            transitions[i, j] = 1 + np.count_nonzero(hashed_transitions == \n",
    "                                                     transition_hash)\n",
    "    # Priors of each chord = total count of pairs starting in that chord.\n",
    "    priors = np.sum(transitions, axis=1)\n",
    "    # Normalize each row of transitions.\n",
    "    transitions /= priors[:, np.newaxis]\n",
    "    # Normalize priors too.\n",
    "    priors /= np.sum(priors)\n",
    "    return transitions, priors\n",
    "\n",
    "def train_chord_models(train_ids):\n",
    "    \"\"\"Train Gaussian models for all chord data from a list of IDs.\n",
    "    \n",
    "    Args:\n",
    "      train_ids:  List of IDs to pass to read_beat_chroma_labels().\n",
    "\n",
    "    Returns:\n",
    "      models: a list of sklearn.mixture.GMM objects, one for each class.\n",
    "      transitions: np.array of size (num_classes, num_classes). \n",
    "        transitions[i, j] is the probability of moving to state j when \n",
    "        starting in state i.\n",
    "      priors: 1D np.array giving the prior probability for each class.\n",
    "\n",
    "    2016-04-03, 2010-04-07 Dan Ellis dpwe@ee.columbia.edu\n",
    "    \"\"\"\n",
    "    features, labels = load_all_features_labels(train_ids)\n",
    "    num_chroma = 12\n",
    "    # We have a major and a minor chord model for each chroma, plus NOCHORD.\n",
    "    num_models = 2 * num_chroma + 1\n",
    "    # Global mean/covariance used for empty models.\n",
    "    global_model = sklearn.mixture.GMM(n_components=1, \n",
    "                                       covariance_type='full')\n",
    "    # Train a background model on all the data, regardless of label.\n",
    "    global_model.fit(features)\n",
    "    # Set up individual models for all chords.\n",
    "    models = []\n",
    "    for model_index in range(num_models):\n",
    "        # labels contains one value in the range 0..24 for each row of \n",
    "        # features.\n",
    "        true_example_rows = np.nonzero(labels == model_index)\n",
    "        if true_example_rows:\n",
    "            model = sklearn.mixture.GMM(n_components=1, \n",
    "                                        covariance_type='full')\n",
    "            model.fit(features[true_example_rows])\n",
    "            models.append(model)\n",
    "        else:\n",
    "            # No training data for this label, so substitute the \n",
    "            # background model.\n",
    "            models.append(global_model)\n",
    "    \n",
    "    transitions, priors = estimate_transitions(labels, num_models)\n",
    "    \n",
    "    return models, transitions, priors\n",
    "\n",
    "def viterbi_path(posteriors, transitions, priors):\n",
    "    \"\"\"Calculate Viterbi (best-cost) path through Markov model.\n",
    "    \n",
    "    Args:\n",
    "      posteriors: np.array sized (num_frames, num_states) giving the \n",
    "        local-match posterior probability of being in state j at time i.\n",
    "      transitions: np.array of (num_states, num_states).  For each row, \n",
    "        transitions(row, col) gives the probability of transitioning from\n",
    "        state row to state col.\n",
    "      priors: np.array of (num_states,) giving prior probability of \n",
    "        each state.    \n",
    "    \"\"\"\n",
    "    num_frames, num_states = posteriors.shape\n",
    "    traceback = np.zeros((num_frames, num_states), dtype=int)\n",
    "    # Normalized best probability-to-date for each state.\n",
    "    best_prob = priors * posteriors[0]\n",
    "    best_prob /= np.sum(best_prob)\n",
    "    for frame in range(1, num_frames):\n",
    "        # Find most likely combination of previous prob-to-path, and \n",
    "        # transition.\n",
    "        possible_transition_scores = (transitions * \n",
    "                                      np.outer(best_prob, posteriors[frame]))\n",
    "        # The max is found for each destination state (column), so the max\n",
    "        # is over all the possible preceding states (rows).\n",
    "        traceback[frame] = np.argmax(possible_transition_scores, axis=0)\n",
    "        best_prob = np.max(possible_transition_scores, axis=0)\n",
    "        best_prob /= np.sum(best_prob)\n",
    "    # Traceback from best final state to get best path.\n",
    "    path = np.zeros(num_frames, dtype=int)\n",
    "    path[-1] = np.argmax(best_prob)\n",
    "    for frame in range(num_frames - 1, 0, -1):\n",
    "        path[frame - 1] = traceback[frame, path[frame]]\n",
    "    return path\n",
    "\n",
    "def recognize_chords(chroma, models, transitions, priors):\n",
    "    \"\"\"Perform chord recognition on chroma feature matrix.\"\"\"\n",
    "    scores = np.array([model.score(chroma) for model in models])\n",
    "    chords = viterbi_path(np.exp(scores.transpose()), transitions, priors)\n",
    "    return chords, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (82359, 12)\n"
     ]
    }
   ],
   "source": [
    "train_list_filename = os.path.join(DATA_DIR, 'trainfilelist.txt')\n",
    "train_ids = read_file_list(train_list_filename)\n",
    "test_list_filename = os.path.join(DATA_DIR, 'testfilelist.txt')\n",
    "test_ids = read_file_list(test_list_filename)\n",
    "\n",
    "# Run the full set of training examples through the model training.\n",
    "models, transitions, priors = train_chord_models(train_ids)\n",
    "# Extract the means from each class's model to illustrate.\n",
    "model_means = np.concatenate([model.means_ for model in models])\n",
    "# Construct a list of names for each of the 25 classes.\n",
    "all_chords = ['-', 'C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', \n",
    "              'B', 'c', 'c#', 'd', 'd#', 'e', 'f', 'f#', 'g', 'g#', 'a', 'a#', 'b']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_id = test_ids[2]\n",
    "beat_times, chroma_features, label_indices = read_beat_chroma_labels(file_id)\n",
    "hyp_chords, scores = recognize_chords(chroma_features, models, transitions, priors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 3 3 3]\n",
      "(637,)\n",
      "[ 0.164  0.444  0.748  1.036  1.324]\n",
      "[[  6.798089     7.47085408  -0.32822758 ...,  -6.63868351  -6.43495699\n",
      "   -1.53859825]\n",
      " [  5.01088989   6.50111516  -4.75959906 ..., -14.08632203 -14.20738854\n",
      "  -13.06187947]\n",
      " [  4.09268478   2.57464549  -5.00723223 ...,  -9.89702973  -8.82294678\n",
      "   -4.36274636]\n",
      " [  2.9408196    2.18335322  11.40085556 ...,   7.9813119    7.30701032\n",
      "   -3.51932142]\n",
      " [  2.90986612   5.17188858  -3.98970498 ..., -14.90898675 -15.54676788\n",
      "  -20.58222363]]\n"
     ]
    }
   ],
   "source": [
    "print(hyp_chords[:5])\n",
    "print(hyp_chords.shape)\n",
    "print(beat_times[:5])\n",
    "print(scores[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Beat tracking example\n",
    "from __future__ import print_function\n",
    "import numpy as np, scipy, matplotlib.pyplot as plt, librosa\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Load the example clip\n",
    "y, sr = librosa.load('/home/py/projects/tmp/tmp.m4a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/numeric.py:482: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "# Separate harmonics and percussives into two waveforms\n",
    "y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "# Beat track on the percussive signal\n",
    "tempo, beat_frames = librosa.beat.beat_track(y=y_percussive, sr=sr)\n",
    "# 4. Convert the frame indices of beat events into timestamps\n",
    "beat_times = librosa.frames_to_time(beat_frames, sr=sr)\n",
    "# Compute chroma features from the harmonic signal\n",
    "chromagram = librosa.feature.chroma_cqt(y=y_harmonic, sr=sr)\n",
    "# Aggregate chroma features between beat events\n",
    "# We'll use the median value of each feature between beat frames\n",
    "beat_chroma = librosa.feature.sync(chromagram, beat_frames, aggregate=np.median)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108,)\n",
      "(109,)\n",
      "[ 0.09287982  0.65015873  1.2306576   1.81115646  2.41487528  3.0185941\n",
      "  3.64553288  4.2492517   4.85297052  5.45668934]\n",
      "[ 0 22 22 22 22 22 22  8  8  8]\n"
     ]
    }
   ],
   "source": [
    "hyp_chords, scores = recognize_chords(beat_chroma.transpose(), models, transitions, priors)\n",
    "print(beat_times.shape)\n",
    "print(hyp_chords.shape)\n",
    "print(beat_times[:10])\n",
    "print(hyp_chords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
